{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "from utils.paths import URL_CONFIG_PATH, BOOKS_PATH, AUTHORS_PATH\n",
    "from utils.enums import URLS, SOUP\n",
    "from utils.regex import NUMBER_PATTERN, LITERAL_PATTERN\n",
    "\n",
    "from scraping.pages import scrape_books\n",
    "from scraping.session import get_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BOOKS_PATH.exists():\n",
    "    visited_books = set(pd.read_csv(BOOKS_PATH)['author_href'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(URL_CONFIG_PATH) as url_config_file:\n",
    "    urls = json.load(url_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def get_with_retry(session, url, retries=5):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            html = session.get(url).text\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f'Error: {e}')\n",
    "            time.sleep(.5)\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Extract user statistics\u001b[39;00m\n\u001b[1;32m     35\u001b[0m user_stats_html \u001b[38;5;241m=\u001b[39m soup_book\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md-flex flex-wrap justify-content-around px-3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m user_stats \u001b[38;5;241m=\u001b[39m \u001b[43muser_stats_html\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\n\u001b[1;32m     37\u001b[0m user_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, re\u001b[38;5;241m.\u001b[39mfindall(NUMBER_PATTERN, user_stats)))\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(user_stats) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store book and author data\n",
    "books_data_dict_list = []\n",
    "authors_data_dict_list = []\n",
    "# give visited_books\n",
    "if BOOKS_PATH.exists():\n",
    "    visited_books = set(pd.read_csv(BOOKS_PATH)['author_href'].unique())\n",
    "else:\n",
    "    visited_books = set()\n",
    "# give visited_authors\n",
    "if AUTHORS_PATH.exists():\n",
    "    visited_authors = set(pd.read_csv(BOOKS_PATH)['author_href'].unique())\n",
    "else:\n",
    "    visited_authors = set()\n",
    "\n",
    "# Loop through pages to scrape book URLs\n",
    "for step in range(1, 334):\n",
    "    books_urls = scrape_books(base_url=urls[URLS.BASE], page_url=urls[URLS.PAGE].format(step=step), session=session)\n",
    "    \n",
    "    # Loop through each book URL to scrape book details\n",
    "    for book_url in books_urls:\n",
    "        # book_html = session.get(book_url).text\n",
    "        book_html = get_with_retry(session, book_url)\n",
    "        soup_book = BeautifulSoup(book_html, SOUP.HTML_PARSER)\n",
    "        \n",
    "        # save the book url\n",
    "        book_full_url = book_url\n",
    "\n",
    "        # Extract author names and hrefs\n",
    "        authors_html = soup_book.find_all('a', class_='link-name d-inline-block')\n",
    "        authors_names = [author.text for author in authors_html]\n",
    "        authors_hrefs = [author['href'] for author in authors_html]\n",
    "\n",
    "        # Scrape author\n",
    "        author_href = authors_hrefs[0]\n",
    "        if author_href not in visited_authors:\n",
    "            author_html = get_with_retry(session, author_href)\n",
    "            authour_soup = BeautifulSoup(author_html, SOUP.HTML_PARSER)\n",
    "            author_author_name = authour_soup.find('div', class_='author-main__header-wrapper').text\n",
    "            author_average_rating = float(authour_soup.find('div', class_='author-box').find('span', class_='rating__avarage').text.replace(',', '.'))\n",
    "            author_number_of_people_read = int(authour_soup.find('div', class_='author-box__readers-col').find('span').text.replace(' ', ''))\n",
    "            author_number_of_people_read, number_of_people_wants_to_read = [int(element.find('span').text.replace(' ', '')) for element in authour_soup.find_all('div', class_='author-box__readers-col')]\n",
    "            author_date_of_birth = pd.to_datetime(authour_soup.find('span', class_='author-info__born').text.split()[-1], format='%d.%m.%Y')\n",
    "            author_number_of_fans = int(authour_soup.find('span', class_='author-box__number').text.replace(' ', ''))\n",
    "            author_number_of_books_written = int(authour_soup.find('div', class_='author-info__count').text)\n",
    "            author_awards_html = authour_soup.find('div', class_='author-info__count author-info__count--awards')\n",
    "            if author_awards_html is not None:\n",
    "                author_number_of_awards = int(author_awards_html.text)\n",
    "            else:\n",
    "                author_number_of_awards = 0\n",
    "            author_data_dict = {\n",
    "                'author_name': author_author_name,\n",
    "                'author_href': author_href,\n",
    "                'author_average_rating': author_average_rating,\n",
    "                'author_number_of_people_read': author_number_of_people_read,\n",
    "                'author_number_of_people_wants_to_read': number_of_people_wants_to_read,\n",
    "                'author_date_of_birth': author_date_of_birth,\n",
    "                'author_number_of_fans': author_number_of_fans,\n",
    "                'author_number_of_books_written': author_number_of_books_written,\n",
    "                'author_number_of_awards': author_number_of_awards\n",
    "            }\n",
    "            authors_data_dict_list.append(author_data_dict)\n",
    "            visited_authors.add(author_href)\n",
    "        # Scrape author end\n",
    "\n",
    "        # scrape publisher\n",
    "        ...\n",
    "        # scrape publisher end\n",
    "        if book_full_url in visited_books:\n",
    "            continue\n",
    "        # Create a dictionary for authors\n",
    "        authors = {}\n",
    "        for index, (author_name, author_href) in enumerate(zip(authors_names, authors_hrefs)):\n",
    "            number = index if index > 0 else ''\n",
    "            authors[f'author{number}'] = author_name\n",
    "            authors[f'author_href{number}'] = author_href\n",
    "        \n",
    "        # Extract book details\n",
    "        pages_html = soup_book.find('span', class_='d-sm-inline-block book-pages book__pages pr-2 mr-2 pr-sm-3 mr-sm-3')\n",
    "        description_html = soup_book.find('div', class_='collapse-content')\n",
    "        description = description_html.text\n",
    "        \n",
    "        # Extract user statistics\n",
    "        user_stats_html = soup_book.find('div', class_='d-flex flex-wrap justify-content-around px-3')\n",
    "        if user_stats_html is None:\n",
    "            number_of_discussions = 0\n",
    "            number_of_user_opinions = 0\n",
    "            number_of_user_ratings = 0\n",
    "        user_stats = user_stats_html.text\n",
    "        user_stats = list(map(int, re.findall(NUMBER_PATTERN, user_stats)))\n",
    "        \n",
    "        if len(user_stats) == 2:\n",
    "            number_of_user_opinions, number_of_user_ratings = user_stats\n",
    "            number_of_discussions = 0\n",
    "        elif len(user_stats) == 3:\n",
    "            number_of_user_opinions, number_of_user_ratings, number_of_discussions = user_stats\n",
    "        \n",
    "        # Extract additional book details\n",
    "        details_dict = dict(zip(\n",
    "            [element.text.strip().rstrip(':') for element in soup_book.find_all('dt')],\n",
    "            [element.text.strip() for element in soup_book.find_all('dd')]\n",
    "        ))\n",
    "        \n",
    "        # Extract on-the-shelf statistics\n",
    "        on_the_shelf_dict_raw = {\n",
    "            re.search(LITERAL_PATTERN, element.text).group().strip(): \"\".join(re.findall(NUMBER_PATTERN, element.text))\n",
    "            for element in soup_book.find_all('li', class_='list-group-item p-0')\n",
    "        }\n",
    "        on_the_shelf_dict = {\n",
    "            'number_of_people_read': on_the_shelf_dict_raw.get('Przeczytane', np.nan),\n",
    "            'number_of_people_has': on_the_shelf_dict_raw.get('Posiadam', np.nan),\n",
    "            'number_of_people_favorite': on_the_shelf_dict_raw.get('Ulubione', np.nan),\n",
    "            'number_of_people_wants_to_read': on_the_shelf_dict_raw.get('Chcę przeczytać', np.nan),\n",
    "            'number_of_people_wants_as_gift': on_the_shelf_dict_raw.get('Chcę w prezencie', np.nan),\n",
    "            'number_of_people_currently_read': on_the_shelf_dict_raw.get('Teraz czytam', np.nan)\n",
    "        }\n",
    "        \n",
    "        # Extract tags\n",
    "        tags = '&'.join([element.text.strip() for element in soup_book.find_all('a', class_='tag')])\n",
    "        \n",
    "        # Extract ratings\n",
    "        ratings_dict = {\n",
    "            f'rating_{element[\"data-rating\"]}': int(\"\".join(re.findall(NUMBER_PATTERN, element.text.strip())))\n",
    "            for element in soup_book.find_all('a', class_='chart-valuebtn btn-link--without-bold plusCountModal')\n",
    "        }\n",
    "        \n",
    "        # Combine all extracted data into a single dictionary\n",
    "        books_data_dict = {\n",
    "            **authors,\n",
    "            'description': description,\n",
    "            'number_of_user_opinions': number_of_user_opinions,\n",
    "            'number_of_user_ratings': number_of_user_ratings,\n",
    "            'number_of_discussions': number_of_discussions,\n",
    "            **details_dict,\n",
    "            **on_the_shelf_dict,\n",
    "            'tags': tags,\n",
    "            **ratings_dict,\n",
    "            'url': book_full_url\n",
    "        }\n",
    "        \n",
    "        # Append the book data dictionary to the list\n",
    "        books_data_dict_list.append(books_data_dict)\n",
    "        \n",
    "        # Sleep for a random time between requests to avoid being blocked\n",
    "        random_sleep_time = random.uniform(0.5, 1.5)\n",
    "        time.sleep(random_sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_href = books_df['author_href'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scape information about authors with retry\n",
    "author_html = get_with_retry(session, author_href)\n",
    "authour_soup = BeautifulSoup(author_html, SOUP.HTML_PARSER)\n",
    "author_author_name = authour_soup.find('div', class_='author-main__header-wrapper').text\n",
    "author_average_rating = float(authour_soup.find('div', class_='author-box').find('span', class_='rating__avarage').text.replace(',', '.'))\n",
    "author_number_of_people_read = int(authour_soup.find('div', class_='author-box__readers-col').find('span').text.replace(' ', ''))\n",
    "author_number_of_people_read, number_of_people_wants_to_read = [int(element.find('span').text.replace(' ', '')) for element in authour_soup.find_all('div', class_='author-box__readers-col')]\n",
    "author_date_of_birth = pd.to_datetime(authour_soup.find('span', class_='author-info__born').text.split()[-1], format='%d.%m.%Y')\n",
    "author_number_of_fans = int(authour_soup.find('span', class_='author-box__number').text.replace(' ', ''))\n",
    "author_number_of_books_written = int(authour_soup.find('div', class_='author-info__count').text)\n",
    "author_awards_html = authour_soup.find('div', class_='author-info__count author-info__count--awards')\n",
    "if author_awards_html is not None:\n",
    "    author_number_of_awards = int(author_awards_html.text)\n",
    "else:\n",
    "    author_number_of_awards = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_number_of_awards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = pd.DataFrame(books_data_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# books_df.to_csv(BOOKS_PATH, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lubimyczytac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
